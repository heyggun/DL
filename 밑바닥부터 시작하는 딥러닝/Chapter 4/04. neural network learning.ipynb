{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e250c99a",
   "metadata": {},
   "source": [
    "## 신경망 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1273257",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79002a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_squares_error(y,t):\n",
    "    return 0.5 * np.sum((y-t)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9753c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09750000000000003\n",
      "0.5975\n"
     ]
    }
   ],
   "source": [
    "# 정답은 2\n",
    "t= [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "# 예1 : '2'일 확률이 가장 높다고 추정함 (0.6)\n",
    "y1 = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "print(sum_squares_error(np.array(y1), np.array(t)))\n",
    "\n",
    "\n",
    "#예2 : '7'일 확률이 가장 높다고 추정함 (0.6)\n",
    "y2 = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0] \n",
    "print(sum_squares_error(np.array(y2), np.array(t)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39496b6",
   "metadata": {},
   "source": [
    "`교차 엔트로피`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b47419d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y,t):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t * np.log(y+delta))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f720bd42",
   "metadata": {},
   "source": [
    "     - 아주 작은 값인 delta를 더함 (np.log()  함수에 0을 입력하면 마이너스 무한대 -inf 가 나오게 됨)\n",
    "     - 아주 작은 값을 더해서 절대 0이 되지 않도록, 마이너스 무한대가 발생하지 않도록 한 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb12dfaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.510825457099338\n",
      "2.302584092994546\n"
     ]
    }
   ],
   "source": [
    "t = [0,0,1,0,0,0,0,0,0,0]\n",
    "y1 = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "print(cross_entropy_error(np.array(y1), np.array(t)))\n",
    "\n",
    "y2 = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "print(cross_entropy_error(np.array(y2), np.array(t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca5ccfcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "(10000, 28, 28)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "import numpy as numpy \n",
    "import tensorflow as tf\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "print(x_train.shape)\n",
    "print(t_train.shape)\n",
    "print(x_test.shape)\n",
    "print(t_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa184e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7066512e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "[36535 23595  3286 46429 40057 53425 44573 22980 40966 25431]\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "[1 4 0 7 5 1 7 4 5 3]\n"
     ]
    }
   ],
   "source": [
    "train_size = x_train.shape[0]\n",
    "batch_size = 10\n",
    "batch_mask = np.random.choice(train_size, batch_size)\n",
    "x_batch = x_train[batch_mask]\n",
    "t_batch = t_train[batch_mask]\n",
    "\n",
    "print(train_size)\n",
    "print(batch_mask)\n",
    "print(x_batch)\n",
    "print(t_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5739a51",
   "metadata": {},
   "source": [
    "    - 훈련 데이터는 60,000개이고 입력 데이터는 784열(28x28)\n",
    "    - 정답 레이블은 10줄로된 데이터\n",
    "    - np.random.choice()로 지정한 범위 수 중에서 무작위로 원하는 개수만 꺼냄 \n",
    "       예) np.random.choice(660000,10) 은 0이상 60000 미만의 수 중에서 무작위로 10개를 골라냄\n",
    "    - 무작위로 뽑은 인덱스로 미니배치를 뽑아내고, 손실함수도 이 미니배치로 계산함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c64f3f",
   "metadata": {},
   "source": [
    "`(배치용) 교차 엔트로피 오차 구하기`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d39ba04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "    \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t *np.log(y+1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c124b3d7",
   "metadata": {},
   "source": [
    "     - y : 신경망 출력, t : 정답 레이블\n",
    "     - y가 1차원이라면, 데이터 하나당 교차 엔트로피 오차를 구할 경우 reshape로 데이터 형상 바꿔줌\n",
    "     - 배치 크기로 나눠 정규화후 이미지 1장당 평균의 교차 엔트로피 오차를 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "736f7bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim==1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "    \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np(log(y[np.arange(batch_size), t] + 1e-7)) / batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7041f6ed",
   "metadata": {},
   "source": [
    "    - 원-핫 인코딩일 때 t가 0인 원소는 교차 엔트로피 오차도 0이므로 계산은 무시해도 좋다는 것이 핵심\n",
    "    - 정답에 해당하는 신경망의 출력만으로 교차 엔트로피 오차를 계산\n",
    "    - 원-핫 인코딩 시 t*np.log(y) 였던 부분을 레이블 표현일 때는 np.log(y[np.arange(batch_size),t])로 구현\n",
    "    - np.log(y[np.arange(batch_size),t]) \n",
    "    - np.arange(batch_size)는 0부터 batch_size -1까지의 배열 생성, \n",
    "      batch_size가 5면, np.arange(batch_size)는 [0,1,2,3,4] 넘파이 배열 생성\n",
    "      t에는 [2,7,0,9,4]와 같이 저장되어 있으므로 y[np.range(batch_size), t]는 각 데이터의 정답 레이블에 해당하는 신경망의 출력을 추출\n",
    "      : 이 예에서는 y[np.ragne(batch_size), t]는 [y[0,2], y[1,7], y[2,0], y[3,9], y[4,4]])인 넘파이 배열 생성\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec853c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8975af2a",
   "metadata": {},
   "source": [
    "`미분 구현`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eaf1cfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 나쁜 구현\n",
    "\n",
    "def numerical_diff(f, x):\n",
    "    h = 1e-50\n",
    "    return (f(x+h) - f(x))/h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78fc010",
   "metadata": {},
   "source": [
    "    - 수치 미분(numerical differenctiation)은 '함수 f'와 '함수 f에 넘길 인수 x' 두 인수를 받음\n",
    "    - h에 가급적 적은 값을 대입하고 싶어서 가능하다면 h를 0을 무한히 가깝게 하고 싶어서 1e-50 이라는 작은 값 이용\n",
    "     : 해당 방식은 반올림 오차(rounding error) 문제를 일으킴 \n",
    "     : 반올림 오차는 작은 값(가령 소수점 8자리 이하)이 생략되어 최종 계산 결과에 오차가 생김"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd24bd50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.float32(1e-50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa8a587",
   "metadata": {},
   "source": [
    "     <개선 1>\n",
    "    - 1e-50을 float32형 (32비트 수동소수점)으로 나타내면 0.0이 되어 올바로 표현할 수 없음\n",
    "    - 너무 작은 값을 이용하면 컴퓨터로 계산하는 데 문제가 됨\n",
    "    - 미세한 값 h을 10-4 정도로 사용하면 좋은 결과를 얻음\n",
    "     \n",
    "     <개선 2>\n",
    "     - 앞의 구현에는 x+h와 x 사이의 함수 f의 차분을 계산하고 있음\n",
    "     - 진정한 미분은 x 위치의 함수의 기울기(접선)에 해당하지만, 위에서 구현한 미분은 (x+h)와 x 사이의 기울기에 해당함\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74fd0547",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_diff(f,x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    return (f(x+h) - f(x-h)) / (2*h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96aa2e24",
   "metadata": {},
   "source": [
    "     - 아주 작은 차분으로 미분하는 것을 `수치 미분` 이라고 함\n",
    "     수식을 전개해 미분하는 것은 '해석적(analytic)' 이라는 말을 이용해 '해석적 해', '해석적으로 미분하다' 로 표현\n",
    "     - 예를 들어 y=x^2의 미분은 dy/dx = 2x로 풀어낼 수 있음\n",
    "       x=2일때 y의 미분은 4가 됨\n",
    "      => 해석적 미분은 오차를 포함하지 않는 '진정한 미분' 값을 구함\n",
    "      \n",
    "     - '해석적 미분'은 수학 시간에 배운 미분,\n",
    "        '수치 미분'은 이를 '근사치'로 계산하는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a751f044",
   "metadata": {},
   "source": [
    "`수치 미분 구현`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23b45558",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_1(x):\n",
    "    return 0.01*x**2 + 0.1*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74d7384f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "x = np.arange(0.0, 20.0, 0.1)  # 0에서 20까지 0.1 간격의 배열 x를 만듦(20은 미포함)\n",
    "y = function_1(x)\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.plot(x,y, label = 'y=0.01*x^2 + 0.1*x')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "229a92db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1999999999990898\n",
      "0.2999999999986347\n"
     ]
    }
   ],
   "source": [
    "# x가 5일 때와 10일 때의 함수의 미분 계산\n",
    "\n",
    "print(numerical_diff(function_1,5))\n",
    "print(numerical_diff(function_1,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e479f449",
   "metadata": {},
   "source": [
    "`편미분`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2fc04281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2  # 혹은 return np.sum(x**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a077efb1",
   "metadata": {},
   "source": [
    "    - 인수 x는 넘파이 배열이라고 가정\n",
    "    - 넘파이 배열의 각 원소를 제곱하고 합을 구하는 구현 (np.sum(x**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4080624c",
   "metadata": {},
   "source": [
    "`x0 = 3, x1 = 4 일 때, x0에 대한 편미분 ∂f/∂x0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f943ba8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.00000000000378\n"
     ]
    }
   ],
   "source": [
    "def function_tmp1(x0):\n",
    "    return x0*x0 + 4.0**2.0\n",
    "\n",
    "print(numerical_diff(function_tmp1, 3.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfb5905",
   "metadata": {},
   "source": [
    "`x0 = 3, x1 = 4 일 때, x1에 대한 편미분 ∂f/∂x1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a55ae87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.999999999999119\n"
     ]
    }
   ],
   "source": [
    "def function_tmp2(x1):\n",
    "    return 3.0**2.0 + x1*x1\n",
    "\n",
    "print(numerical_diff(function_tmp2, 4.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bc4334",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87391f09",
   "metadata": {},
   "source": [
    "`기울기 구현`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ebc88f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4  # 0.0001\n",
    "    grad = np.zeros_like(x) # x와 형상이 같은 배열을 생성\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        # f(x+h) 계산\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        # f(x-h)계산\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "        \n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp_val # 값 복원\n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69192a13",
   "metadata": {},
   "source": [
    "     - numerical_gradient(f,x) 함수의 인수인 f는 함수이고, x는 넘파이 배열\n",
    "     넘파이 배열 x의 각 원소에 대해서 수치 미분을 구함\n",
    "     - 세 점 (3,4), (0,2), (3,0) 에서의 기울기 구함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a18518a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6. 8.]\n",
      "[0. 4.]\n",
      "[6. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(numerical_gradient(function_2, np.array([3.0, 4.0])))\n",
    "print(numerical_gradient(function_2, np.array([0.0, 2.0])))\n",
    "print(numerical_gradient(function_2, np.array([3.0, 0.0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d348b2e6",
   "metadata": {},
   "source": [
    "    - (x0, x1)의 각 점에서의 기울기를 계산\n",
    "    - 점 (3,4)의 기울기는 (6,8) / 점 (0,2)의 기울기는 (0,4) / 점 (3,0)의 기울기는 (6,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e537ce",
   "metadata": {},
   "source": [
    "`경사하강법 구현`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "188f8978",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    x = init_x\n",
    "    \n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr*grad\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e966d4",
   "metadata": {},
   "source": [
    "    - 인수 f는 최적화 하려는 함수, init_x : 초기값, lr : learning rate를 의미하는 학습률\n",
    "    step_num : 경사법에 따른 반복 횟수\n",
    "    - 함수의 기울기는 numerical_gradient(f,x)로 구하고, 기울기에 학습률을 곱한 값으로 갱신하는 처리를 step_num번 반복"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac19b65d",
   "metadata": {},
   "source": [
    "`경사법으로 f(x0, x1)=x0^2 +x1^2 의 최솟값 구하기`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "36d1623a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-6.11110793e-10  8.14814391e-10]\n"
     ]
    }
   ],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "print(gradient_descent(function_2, init_x=init_x, lr=0.1, step_num=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d717d008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_graph(f, init_x, lr=0.01, step_num=100):\n",
    "    x = init_x\n",
    "    x_history = []\n",
    "    for i in range(step_num):\n",
    "        x_history.append(x.copy())\n",
    "        grad = numerical_gradient(f,x)\n",
    "        x -= lr*grad\n",
    "    return x, np.array(x_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d3352282",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.00000000e+00  4.00000000e+00]\n",
      " [-2.40000000e+00  3.20000000e+00]\n",
      " [-1.92000000e+00  2.56000000e+00]\n",
      " [-1.53600000e+00  2.04800000e+00]\n",
      " [-1.22880000e+00  1.63840000e+00]\n",
      " [-9.83040000e-01  1.31072000e+00]\n",
      " [-7.86432000e-01  1.04857600e+00]\n",
      " [-6.29145600e-01  8.38860800e-01]\n",
      " [-5.03316480e-01  6.71088640e-01]\n",
      " [-4.02653184e-01  5.36870912e-01]\n",
      " [-3.22122547e-01  4.29496730e-01]\n",
      " [-2.57698038e-01  3.43597384e-01]\n",
      " [-2.06158430e-01  2.74877907e-01]\n",
      " [-1.64926744e-01  2.19902326e-01]\n",
      " [-1.31941395e-01  1.75921860e-01]\n",
      " [-1.05553116e-01  1.40737488e-01]\n",
      " [-8.44424930e-02  1.12589991e-01]\n",
      " [-6.75539944e-02  9.00719925e-02]\n",
      " [-5.40431955e-02  7.20575940e-02]\n",
      " [-4.32345564e-02  5.76460752e-02]\n",
      " [-3.45876451e-02  4.61168602e-02]\n",
      " [-2.76701161e-02  3.68934881e-02]\n",
      " [-2.21360929e-02  2.95147905e-02]\n",
      " [-1.77088743e-02  2.36118324e-02]\n",
      " [-1.41670994e-02  1.88894659e-02]\n",
      " [-1.13336796e-02  1.51115727e-02]\n",
      " [-9.06694365e-03  1.20892582e-02]\n",
      " [-7.25355492e-03  9.67140656e-03]\n",
      " [-5.80284393e-03  7.73712525e-03]\n",
      " [-4.64227515e-03  6.18970020e-03]\n",
      " [-3.71382012e-03  4.95176016e-03]\n",
      " [-2.97105609e-03  3.96140813e-03]\n",
      " [-2.37684488e-03  3.16912650e-03]\n",
      " [-1.90147590e-03  2.53530120e-03]\n",
      " [-1.52118072e-03  2.02824096e-03]\n",
      " [-1.21694458e-03  1.62259277e-03]\n",
      " [-9.73555661e-04  1.29807421e-03]\n",
      " [-7.78844529e-04  1.03845937e-03]\n",
      " [-6.23075623e-04  8.30767497e-04]\n",
      " [-4.98460498e-04  6.64613998e-04]\n",
      " [-3.98768399e-04  5.31691198e-04]\n",
      " [-3.19014719e-04  4.25352959e-04]\n",
      " [-2.55211775e-04  3.40282367e-04]\n",
      " [-2.04169420e-04  2.72225894e-04]\n",
      " [-1.63335536e-04  2.17780715e-04]\n",
      " [-1.30668429e-04  1.74224572e-04]\n",
      " [-1.04534743e-04  1.39379657e-04]\n",
      " [-8.36277945e-05  1.11503726e-04]\n",
      " [-6.69022356e-05  8.92029808e-05]\n",
      " [-5.35217885e-05  7.13623846e-05]\n",
      " [-4.28174308e-05  5.70899077e-05]\n",
      " [-3.42539446e-05  4.56719262e-05]\n",
      " [-2.74031557e-05  3.65375409e-05]\n",
      " [-2.19225246e-05  2.92300327e-05]\n",
      " [-1.75380196e-05  2.33840262e-05]\n",
      " [-1.40304157e-05  1.87072210e-05]\n",
      " [-1.12243326e-05  1.49657768e-05]\n",
      " [-8.97946606e-06  1.19726214e-05]\n",
      " [-7.18357285e-06  9.57809713e-06]\n",
      " [-5.74685828e-06  7.66247770e-06]\n",
      " [-4.59748662e-06  6.12998216e-06]\n",
      " [-3.67798930e-06  4.90398573e-06]\n",
      " [-2.94239144e-06  3.92318858e-06]\n",
      " [-2.35391315e-06  3.13855087e-06]\n",
      " [-1.88313052e-06  2.51084069e-06]\n",
      " [-1.50650442e-06  2.00867256e-06]\n",
      " [-1.20520353e-06  1.60693804e-06]\n",
      " [-9.64162827e-07  1.28555044e-06]\n",
      " [-7.71330261e-07  1.02844035e-06]\n",
      " [-6.17064209e-07  8.22752279e-07]\n",
      " [-4.93651367e-07  6.58201823e-07]\n",
      " [-3.94921094e-07  5.26561458e-07]\n",
      " [-3.15936875e-07  4.21249167e-07]\n",
      " [-2.52749500e-07  3.36999333e-07]\n",
      " [-2.02199600e-07  2.69599467e-07]\n",
      " [-1.61759680e-07  2.15679573e-07]\n",
      " [-1.29407744e-07  1.72543659e-07]\n",
      " [-1.03526195e-07  1.38034927e-07]\n",
      " [-8.28209562e-08  1.10427942e-07]\n",
      " [-6.62567649e-08  8.83423532e-08]\n",
      " [-5.30054119e-08  7.06738826e-08]\n",
      " [-4.24043296e-08  5.65391061e-08]\n",
      " [-3.39234636e-08  4.52312849e-08]\n",
      " [-2.71387709e-08  3.61850279e-08]\n",
      " [-2.17110167e-08  2.89480223e-08]\n",
      " [-1.73688134e-08  2.31584178e-08]\n",
      " [-1.38950507e-08  1.85267343e-08]\n",
      " [-1.11160406e-08  1.48213874e-08]\n",
      " [-8.89283245e-09  1.18571099e-08]\n",
      " [-7.11426596e-09  9.48568795e-09]\n",
      " [-5.69141277e-09  7.58855036e-09]\n",
      " [-4.55313022e-09  6.07084029e-09]\n",
      " [-3.64250417e-09  4.85667223e-09]\n",
      " [-2.91400334e-09  3.88533778e-09]\n",
      " [-2.33120267e-09  3.10827023e-09]\n",
      " [-1.86496214e-09  2.48661618e-09]\n",
      " [-1.49196971e-09  1.98929295e-09]\n",
      " [-1.19357577e-09  1.59143436e-09]\n",
      " [-9.54860614e-10  1.27314749e-09]\n",
      " [-7.63888491e-10  1.01851799e-09]]\n"
     ]
    }
   ],
   "source": [
    "x, x_history = gradient_descent_graph(function_2, init_x= np.array([-3.0, 4.0]), lr=0.1, step_num=100)\n",
    "\n",
    "print(x_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "413b7e4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x, x_history = gradient_descent_graph(function_2, init_x= np.array([-3.0, 4.0]), lr=0.1, step_num=100)\n",
    "\n",
    "plt.plot( [-5, 5], [0,0], '--b')\n",
    "plt.plot( [0,0], [-5, 5], '--b')\n",
    "plt.plot(x_history[:,0], x_history[:,1], 'o')\n",
    "\n",
    "plt.xlim(-3.5, 3.5)\n",
    "plt.ylim(-4.5, 4.5)\n",
    "plt.xlabel(\"X0\")\n",
    "plt.ylabel(\"X1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3e7979",
   "metadata": {},
   "source": [
    "`학습률에 따른 비교`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5ef75ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.99999994  3.99999992]\n"
     ]
    }
   ],
   "source": [
    "# 학습률이 너무 큰 경우 lr = 1.0\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "print(gradient_descent(function_2, init_x = init_x, lr=10.0, step_num=100))\n",
    "\n",
    "# 학습률이 너무 작은 경우 lr = 1e-10\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "print(gradient_descent(function_2, init_x = init_x, lr=1e-10, step_num=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0b07dcc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEGCAYAAABsLkJ6AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAS8klEQVR4nO3df5BdZX3H8c/HNGLwx9CSbRGyaZwKOzrIhHoHJVpLIUqgiWhBIlNSqa0boFrSCVCToI4GUQtoZtqGybY4scAomQAqNCkklAx1AsomrvwKoYw1JtGWRU2VIdMS+PaPc7ck+yPZe87uffbs837NPHPuuefmns/sbM53n+c5PxwRAgDk51WpAwAA0qAAAECmKAAAkCkKAABkigIAAJn6tdQBWjF9+vSYNWtW6hgAUCvbtm17LiI6Br9fqwIwa9Ys9fb2po4BHGL37mLZ2Zk2BzAS27uGe79WBQCYiBYtKpZbtiSNAbSMOQAAyBQFAAAyRQEAgExRAAAgU0wCAxUtXZo6AVAOBQCoaMGC1AmAcpIXANtTJPVK2hsR81Nk+Ob39+r6e3fqJ/v26/hjpumqs7v0gVNPSBEFNbRzZ7Hs6kqbA2hV8gIg6QpJOyS9IcXOv/n9vVp252Pa/+JLkqS9+/Zr2Z2PSRJFAKOyeHGx5DoA1E3SSWDbMyT9oaR/TJXh+nt3/v/Bf8D+F1/S9ffuTJQIANoj9VlAqyRdLenlkT5gu9t2r+3e/v7+MQ/wk337W3ofACaLZAXA9nxJz0bEtsN9LiJ6IqIREY2OjiH3Mqrs+GOmtfQ+AEwWKXsA75L0fts/kvQNSWfavrXdIa46u0vTpk455L1pU6foqrOZ0QMwuSWbBI6IZZKWSZLtMyRdGREXtzvHwEQvZwGhrGuuSZ0AKGcinAWU3AdOPYEDPkqbOzd1AqCcCVEAImKLpC2JYwCl9PUVy9mzU6YAWjchCgBQZ0uWFEuuA0DdpD4NFACQCAUAADJFAQCATFEAACBTTAIDFV13XeoEQDkUAKCiOXNSJwDKYQgIqGjr1qIBdUMPAKho+fJiyXUAqBt6AACQKQoAAGSKAgAAmaIAAECmmAQGKlq1KnUCoBwKAFARt4FGXaV8JvBrbH/P9g9sP2H7s6myAFVs3lw0oG5S9gD+R9KZEfG87amSvmN7Y0Q8nDAT0LJrry2WPBkMdZPymcAh6fnm6tRmi1R5ACA3Sc8Csj3Fdp+kZyVtiojvpswDADlJWgAi4qWImC1phqTTbJ88+DO2u2332u7t7+9ve0YAmKwmxHUAEbFP0gOS5g2zrSciGhHR6OjoaHs2AJisks0B2O6Q9GJE7LM9TdJ7JX0pVR6grDVrUicAykl5FtAbJX3N9hQVPZF1EXFPwjxAKV1dqRMA5aQ8C+hRSaem2j8wVu6+u1guWJA2B9AqrgQGKrrxxmJJAUDdTIhJYABA+1EAACBTFAAAyBQFAAAyxSQwUNEtt6ROAJRDAQAq6uxMnQAohyEgoKLbby8aUDf0AICKbrqpWC5cmDYH0Cp6AACQKQoAAGSKAgAAmaIAAECmmAQGKlq/PnUCoBwKAFDR9OmpEwDlMAQEVLR2bdGAuklWAGx32n7A9pO2n7B9RaosQBUUANRVyiGgA5KWRsR226+XtM32poh4MmEmAMhGsh5ARPw0IrY3X/9K0g5JJ6TKAwC5mRBzALZnqXg+8HeH2dZtu9d2b39/f9uzAcBklbwA2H6dpDskLYmIXw7eHhE9EdGIiEZHR0f7AwLAJJX0NFDbU1Uc/G+LiDtTZgHK2rAhdQKgnGQFwLYl3SxpR0R8OVUOoKqjj06dACgn5RDQuyQtknSm7b5mOzdhHqCU1auLBtRNsh5ARHxHklPtHxgr69YVy8svT5sDaFXySWAAQBoUAADIFAUAADJFAQCATHE7aKCiLVtSJwDKoQcAAJmiAAAV3XBD0YC6oQAAFd1zT9GAuqEAAECmKAAAkCkKAABkitNAgYqmTUudACiHAgBUtHFj6gRAOQwBAUCmKABARStXFg2om6QFwPZXbT9r+/GUOYAq7r+/aEDdpO4BrJU0L3EGAMhS0gIQEQ9K+nnKDACQq9Q9gCOy3W2713Zvf39/6jgAMGlM+AIQET0R0YiIRkdHR+o4wBDHHls0oG64DgCo6I47UicAypnwPQAAwPhIfRro1yU9JKnL9h7bf5YyD1DGsmVFA+om6RBQRFyUcv/AWHjoodQJgHIYAgKATFEAACBTFAAAyBSngQIVzZiROgFQDgUAqOjWW1MnAMphCAgAMkUBACpasqRoQN0wBARU1NeXOgFQDj0AAMgUBQAAMkUBAIBMMQcAVHTSSakTAOVQAICKenpSJwDKYQgIADJFAQAq6u4uGlA3pQqA7feOxc5tz7O90/Yztj85Ft8JtNvTTxcNqJuyPYCbq+7Y9hRJfy/pHElvlXSR7bdW/V4AwOiMOAls+9sjbZJ07Bjs+zRJz0TED5v7+4ak8yQ9OdI/2LlT2rpVmjOnWC5fPvQzq1ZJs2dLmzdL1147dPuaNVJXl3T33dKNNw7dfsstUmendPvt0k03Dd2+fr00fbq0dm3RBtuwQTr6aGn1amnduqHbt2wpljfcIN1zz6Hbpk2TNm4sXq9cKd1//6Hbjz32lQeQL1s29ElUM2a8cmOyJUuGXqF60kmvTFh2dw/9q3X27OLnJ0kXXyzt2XPo9tNPl77wheL1+edLP/vZodvPOkv61KeK1+ecI+3ff+j2+fOlK68sXp9xhoa48ELp8sulF16Qzj136PZLLinac89JF1wwdPtll0kLF0q7d0uLFg3dvnSptGBB8Xu0ePHQ7ddcI82dW/zchru1w3XXDf+7N/Bz7uvjd0/id6+dv3sDjnTcG8nhzgL6PUkXS3p+0PtWcfCu6gRJuw9a3yPpHYM/ZLtbUrckHXXUKWOwWwCAJDkiht9gb5T0NxHxwDDbHoyI91TasX2BpHkR8efN9UWS3hERHx/p3zQajejt7a2yW2DMDfzFNvAXLDDR2N4WEY3B7x+uB7A4In48wrYVY5Bpr6TOg9ZnNN8DaoUDP+rqcJPAW2xf3ZyslSTZ/i3bt0r6yhjs+xFJJ9p+k+1XS/qwpJHmHQAAY+xwBeDtkn5HUp/tM21fIel7kh7SGMwBRMQBSR+XdK+kHZLWRcQTVb8XaLeLLy4aUDcjDgFFxC8kLW4e+DdL+omkd0bEnpH+TasiYoOkDWP1fUAKg89YAepixB6A7WNsr5H0p5LmSVovaaPtM9sVDgAwfg43Cbxd0mpJf9EcrrnP9mxJq23vioiL2hEQADA+DlcA3jN4uCci+iTNsf2xcU0FABh3h5sDGHFkMyL+YXziAPVz+umpEwDl8DwAoKKBWxQAdcPtoAEgUxQAoKLzzy8aUDcMAQEVDb4zJVAX9AAAIFMUAADIFAUAADLFHABQ0VlnpU4AlEMBACoaeBQhUDcMAQFApigAQEXnnFM0oG6SFADbH7L9hO2XbQ95TiVQJ/v3Fw2om1Q9gMcl/ZGkBxPtHwCyl2QSOCJ2SJLtFLsHAKgGcwC2u2332u7t7+9PHQcAJo1x6wHY3izpuGE2rYiIb432eyKiR1KPJDUajRijeMCYmT8/dQKgnHErABExd7y+G5hIrrwydQKgnAk/BAQAGB+pTgP9oO09kk6X9M+2702RAxgLZ5xRNKBuUp0FdJeku1LsGwBQYAgIADJFAQCATFEAACBT3A4aqOjCC1MnAMqhAAAVXX556gRAOQwBARW98ELRgLqhBwBUdO65xXLLlqQxgJbRAwCATFEAACBTFAAAyBQFAAAyxSQwUNEll6ROAJRDAQAqogCgrhgCAip67rmiAXVDDwCo6IILiiXXAaBuUj0Q5nrbT9l+1PZdto9JkQMAcpZqCGiTpJMj4hRJT0taligHAGQrSQGIiPsi4kBz9WFJM1LkAICcTYRJ4I9K2jjSRtvdtntt9/b397cxFgBMbuM2CWx7s6Tjhtm0IiK+1fzMCkkHJN020vdERI+kHklqNBoxDlGBSi67LHUCoJxxKwARMfdw221fImm+pLMiggM7amvhwtQJgHKSnAZqe56kqyX9fkRwJ3XU2u7dxbKzM20OoFWprgP4O0lHSdpkW5IejohLE2UBKlm0qFhyHQDqJkkBiIg3p9gvAOAVE+EsIABAAhQAAMgUBQAAMsXN4ICKli5NnQAohwIAVLRgQeoEQDkMAQEV7dxZNKBu6AEAFS1eXCy5DgB1Qw8AADJFAQCATFEAACBTFAAAyBSTwEBF11yTOgFQDgUAqGjuYZ98AUxcDAEBFfX1FQ2oG3oAQEVLlhRLrgNA3STpAdheaftR232277N9fIocAJCzVENA10fEKRExW9I9kj6dKAcAZCtJAYiIXx60+lpJPBQeANos2RyA7c9L+hNJ/y3pD1LlAIBcOWJ8/vi2vVnSccNsWhER3zroc8skvSYiPjPC93RL6pakmTNnvn3Xrl3jERcobevWYjlnTtocwEhsb4uIxpD3x6sAjJbtmZI2RMTJR/pso9GI3t7eNqQCgMljpAKQ6iygEw9aPU/SUylyAGNh69ZXegFAnaSaA/ii7S5JL0vaJenSRDmAypYvL5ZcB4C6SVIAIuL8FPsFALyCW0EAQKYoAACQKQoAAGSKm8EBFa1alToBUA4FAKho9uzUCYByGAICKtq8uWhA3dADACq69tpiyZPBUDf0AAAgUxQAAMgUBQAAMkUBAIBMMQkMVLRmTeoEQDkUAKCirq7UCYByGAICKrr77qIBdUMPAKjoxhuL5YIFaXMAraIHAACZSloAbC+1Hbanp8wBADlKVgBsd0p6n6Qfp8oAADlL2QP4iqSrJUXCDACQrSSTwLbPk7Q3In5g+0if7ZbULUkzZ85sQzqgNbfckjoBUM64FQDbmyUdN8ymFZKWqxj+OaKI6JHUI0mNRoPeAiaczs7UCYByxq0ARMSwN8e1/TZJb5I08Nf/DEnbbZ8WEf85XnmA8XL77cVy4cK0OYBWtX0IKCIek/SbA+u2fySpERHPtTsLMBZuuqlYUgBQN1wHAACZSn4lcETMSp0BAHJEDwAAMkUBAIBMJR8CAupu/frUCYByKABARdO5kxVqiiEgoKK1a4sG1A0FAKiIAoC6ckR97q5gu1/SrnHcxXRJdb4gjfzp1Dm7RP7Uxjv/b0dEx+A3a1UAxpvt3ohopM5RFvnTqXN2ifyppcrPEBAAZIoCAACZogAcqid1gIrIn06ds0vkTy1JfuYAACBT9AAAIFMUAADIFAVgENsrbT9qu8/2fbaPT51ptGxfb/upZv67bB+TOlMrbH/I9hO2X7Zdm1P6bM+zvdP2M7Y/mTpPK2x/1fazth9PnaUM2522H7D9ZPN354rUmUbL9mtsf8/2D5rZP9v2DMwBHMr2GyLil83XfynprRFxaeJYo2L7fZL+NSIO2P6SJEXEXyeONWq23yLpZUlrJF0ZEb2JIx2R7SmSnpb0Xkl7JD0i6aKIeDJpsFGy/R5Jz0v6p4g4OXWeVtl+o6Q3RsR226+XtE3SB+rw83fxTNzXRsTztqdK+o6kKyLi4XZloAcwyMDBv+m1kmpTISPivog40Fx9WMXzlmsjInZExM7UOVp0mqRnIuKHEfG/kr4h6bzEmUYtIh6U9PPUOcqKiJ9GxPbm619J2iHphLSpRicKzzdXpzZbW483FIBh2P687d2S/ljSp1PnKemjkjamDpGBEyTtPmh9j2pyAJpsbM+SdKqk7yaOMmq2p9juk/SspE0R0dbsWRYA25ttPz5MO0+SImJFRHRKuk3Sx9OmPdSRsjc/s0LSARX5J5TR5AdaZft1ku6QtGRQL35Ci4iXImK2it76abbbOgyX5fMAImLuKD96m6QNkj4zjnFacqTsti+RNF/SWTEBJ3ha+NnXxV5JnQetz2i+hzZpjp/fIem2iLgzdZ4yImKf7QckzZPUtgn5LHsAh2P7xINWz5P0VKosrbI9T9LVkt4fES+kzpOJRySdaPtNtl8t6cOSvp04UzaaE6k3S9oREV9OnacVtjsGztSzPU3FiQRtPd5wFtAgtu+Q1KXibJRdki6NiFr8RWf7GUlHSfpZ862H63IGkyTZ/qCkv5XUIWmfpL6IODtpqFGwfa6kVZKmSPpqRHw+baLRs/11SWeouB3xf0n6TETcnDRUC2y/W9K/SXpMxf9ZSVoeERvSpRod26dI+pqK35tXSVoXEZ9rawYKAADkiSEgAMgUBQAAMkUBAIBMUQAAIFMUAADIFAUAaEHz7pP/Yfs3muu/3lyfZfsjtv+92T6SOitwJJwGCrTI9tWS3hwR3bbXSPqRijuY9kpqqLih1zZJb4+IXyQLChwBPQCgdV+R9E7bSyS9W9INks5WcTOvnzcP+ptUXNYPTFhZ3gsIqCIiXrR9laR/kfS+5jp3BUXt0AMAyjlH0k8l1e4hKsAACgDQItuzVdy4652S/qr5VCruCoraYRIYaEHz7pNbJX06IjbZ/oSKQvAJFRO/v9v86HYVk8C1fdoWJj96AEBrPibpxxGxqbm+WtJbJL1N0koVt4d+RNLnOPhjoqMHAACZogcAAJmiAABApigAAJApCgAAZIoCAACZogAAQKYoAACQqf8DwlsuLV+6/nYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 학습률이 너무 큰 경우\n",
    "\n",
    "x, x_history = gradient_descent_graph(function_2, init_x= np.array([-3.0, 4.0]), lr=10.0, step_num=100)\n",
    "\n",
    "plt.plot( [-5, 5], [0,0], '--b')\n",
    "plt.plot( [0,0], [-5, 5], '--b')\n",
    "plt.plot(x_history[:,0], x_history[:,1], 'o')\n",
    "\n",
    "plt.xlim(-3.5, 3.5)\n",
    "plt.ylim(-4.5, 4.5)\n",
    "plt.xlabel(\"X0\")\n",
    "plt.ylabel(\"X1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "867d85a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 학습률이 너무 작은 경우\n",
    "\n",
    "x, x_history = gradient_descent_graph(function_2, init_x= np.array([-3.0, 4.0]), lr=1e-10, step_num=100)\n",
    "\n",
    "plt.plot( [-5, 5], [0,0], '--b')\n",
    "plt.plot( [0,0], [-5, 5], '--b')\n",
    "plt.plot(x_history[:,0], x_history[:,1], 'o')\n",
    "\n",
    "plt.xlim(-3.5, 3.5)\n",
    "plt.ylim(-4.5, 4.5)\n",
    "plt.xlabel(\"X0\")\n",
    "plt.ylabel(\"X1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2e024d",
   "metadata": {},
   "source": [
    "`기울기를 구하는 simpleNet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ca45e824",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return y.T \n",
    "\n",
    "    x = x - np.max(x) # 오버플로 대책\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ba335930",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _numerical_gradient_1d(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x) # f(x+h)\n",
    "        \n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x) # f(x-h)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        \n",
    "        x[idx] = tmp_val # 값 복원\n",
    "        \n",
    "    return grad\n",
    "\n",
    "\n",
    "def numerical_gradient_2d(f, X):\n",
    "    if X.ndim == 1:\n",
    "        return _numerical_gradient_1d(f, X)\n",
    "    else:\n",
    "        grad = np.zeros_like(X)\n",
    "        \n",
    "        for idx, x in enumerate(X):\n",
    "            grad[idx] = _numerical_gradient_1d(f, x)\n",
    "        \n",
    "        return grad\n",
    "\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x) # f(x+h)\n",
    "        \n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x) # f(x-h)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        \n",
    "        x[idx] = tmp_val # 값 복원\n",
    "        it.iternext()   \n",
    "        \n",
    "    return grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "41b98f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class simpleNet():\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3) # 정규 분포로 초기화\n",
    "        \n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y,t)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f6cffdd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.01925375 -0.74715466 -1.28994248]\n",
      " [-0.30877078  1.03290267  1.00377919]]\n",
      "[-0.28944595  0.48131961  0.12943579]\n",
      "1\n",
      "1.12477516169295\n",
      "[[ 0.12815906  0.27700559 -0.40516465]\n",
      " [ 0.19223859  0.41550839 -0.60774698]]\n"
     ]
    }
   ],
   "source": [
    "net = simpleNet()\n",
    "print(net.W)\n",
    "\n",
    "x = np.array([0.6, 0.9])\n",
    "p = net.predict(x)\n",
    "print(p)\n",
    "print(np.argmax(p))\n",
    "\n",
    "t = np.array([0,0,1]) # 정답 레이블\n",
    "print(net.loss(x,t))\n",
    "\n",
    "def f(W):\n",
    "    return net.loss(x,t)\n",
    "\n",
    "dW = numerical_gradient(f, net.W)\n",
    "print(dW) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ca37df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3b96ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74d7c9b7",
   "metadata": {},
   "source": [
    "`2층 신경망 클래스 구현하기`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "342ad0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return y.T \n",
    "\n",
    "    x = x - np.max(x) # 오버플로 대책\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "cc6db20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _numerical_gradient_1d(f,x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeors_like(x)\n",
    "\n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] =  float(tmp_val) + h\n",
    "        fxh1 = f(x)\n",
    "\n",
    "        x[idx] = float(tmp_val) - h\n",
    "        fxh2 = f(x)\n",
    "        grad[idx] = (fxh1 - fxh2) / 2*h\n",
    "\n",
    "        x[idx] = tmp_val\n",
    "\n",
    "    return grad           \n",
    "\n",
    "\n",
    "def numerical_gradient_2d(X):\n",
    "    if X.dim==1:\n",
    "        return self._numerical_gradient_1d(f, X)\n",
    "\n",
    "    else:\n",
    "        grad = np.zeros_like(x)\n",
    "\n",
    "        for idx, x in enumerate(X):\n",
    "            grad[idx] = self._numerical_gradeint_1d(f,x)\n",
    "    return grad\n",
    "\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x) # f(x+h)\n",
    "\n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x) # f(x-h)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "\n",
    "        x[idx] = tmp_val # 값 복원\n",
    "        it.iternext()   \n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b0d804b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        \n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "        \n",
    "    def predict(self,x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    # x : 입력데이터, t : 정답 레이블\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return cross_entropy_error(y,t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y==t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    # x : 입력데이터, t : 정답레이블\n",
    "    \n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W : self.loss(x,t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "bdcd4c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 100)\n",
      "(100,)\n",
      "(100, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "net = TwoLayerNet(input_size= 784, hidden_size=100, output_size=10)\n",
    "\n",
    "print(net.params['W1'].shape)\n",
    "print(net.params['b1'].shape)\n",
    "print(net.params['W2'].shape)\n",
    "print(net.params['b2'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b5f3738c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.09522548 0.10626582 0.09337892 0.0996541  0.09084703 0.10867615\n",
      "  0.09339934 0.10952082 0.10432738 0.09870496]\n",
      " [0.09530596 0.10679678 0.09374079 0.10000874 0.09018648 0.10847678\n",
      "  0.09314295 0.10949702 0.10392098 0.09892354]\n",
      " [0.09546692 0.10646829 0.09364807 0.09963517 0.0909209  0.10828131\n",
      "  0.09324958 0.1091469  0.10433268 0.09885018]\n",
      " [0.09552495 0.10632132 0.0936813  0.09973139 0.0905162  0.10829941\n",
      "  0.09340522 0.10946482 0.10430547 0.09874992]\n",
      " [0.09549928 0.10648586 0.09372222 0.09980761 0.09081497 0.10840381\n",
      "  0.09341487 0.1095198  0.10346792 0.09886366]\n",
      " [0.09521038 0.10631824 0.09354034 0.09976689 0.09071761 0.10844598\n",
      "  0.09342879 0.10959077 0.10411634 0.09886466]\n",
      " [0.09542554 0.10614017 0.0937984  0.09949338 0.0907502  0.10848984\n",
      "  0.09296778 0.10975967 0.10431602 0.09885901]\n",
      " [0.09523043 0.10620395 0.09375456 0.09937092 0.09067727 0.10907379\n",
      "  0.09350063 0.10916065 0.10416495 0.09886285]\n",
      " [0.0956111  0.10649426 0.09365349 0.09946434 0.09064288 0.10835646\n",
      "  0.09317484 0.10983189 0.1041185  0.09865223]\n",
      " [0.09522166 0.10635188 0.0933644  0.09936755 0.09083196 0.10854427\n",
      "  0.09360175 0.10927119 0.10463374 0.0988116 ]\n",
      " [0.09567269 0.10622969 0.09360754 0.09933708 0.09083756 0.10856849\n",
      "  0.09329552 0.10954842 0.10404071 0.09886231]\n",
      " [0.09524482 0.10623365 0.09320695 0.09961918 0.0907127  0.10861508\n",
      "  0.09371836 0.10936049 0.10444709 0.09884168]\n",
      " [0.09534474 0.10645722 0.09350124 0.09938382 0.09061761 0.10836258\n",
      "  0.09312115 0.10970099 0.10459837 0.09891228]\n",
      " [0.09566478 0.10642344 0.09338342 0.09954869 0.09063328 0.10836306\n",
      "  0.09341823 0.1096704  0.10417982 0.09871488]\n",
      " [0.09551152 0.10646333 0.09361512 0.09973157 0.09072801 0.10817523\n",
      "  0.09317447 0.10949912 0.10412184 0.09897979]\n",
      " [0.09524762 0.10607256 0.09323709 0.09985848 0.09038156 0.10870006\n",
      "  0.09340625 0.1097484  0.10431104 0.09903692]\n",
      " [0.09541198 0.1061752  0.09355443 0.09991658 0.09076231 0.10849385\n",
      "  0.09313392 0.1094182  0.10434413 0.09878941]\n",
      " [0.09558201 0.10626387 0.09344476 0.09981543 0.09066704 0.10840785\n",
      "  0.09321756 0.1095768  0.10427223 0.09875245]\n",
      " [0.09568667 0.1060441  0.09352513 0.09968295 0.09089214 0.10806045\n",
      "  0.09319466 0.10953521 0.104376   0.0990027 ]\n",
      " [0.09522397 0.10617788 0.09349511 0.09981599 0.09089119 0.10849218\n",
      "  0.09349531 0.10950916 0.10404783 0.09885138]\n",
      " [0.09564273 0.1063429  0.09364491 0.09943393 0.09082342 0.10829989\n",
      "  0.09315857 0.10955287 0.10436738 0.09873341]\n",
      " [0.0954686  0.10619432 0.09364482 0.09957401 0.09076855 0.10815102\n",
      "  0.09351725 0.10954845 0.10436761 0.09876537]\n",
      " [0.09541318 0.10630131 0.09383734 0.09979375 0.0908461  0.108427\n",
      "  0.0931422  0.10905462 0.1042659  0.09891859]\n",
      " [0.09506965 0.10603487 0.09383226 0.09943155 0.09076155 0.10894521\n",
      "  0.0932741  0.10996052 0.10426811 0.09842219]\n",
      " [0.09560022 0.10617241 0.0937462  0.0997872  0.09043334 0.10844275\n",
      "  0.09342313 0.10971659 0.10397761 0.09870056]\n",
      " [0.09563824 0.10638304 0.09375335 0.09966601 0.09056516 0.10842087\n",
      "  0.09316371 0.10940362 0.10408195 0.09892403]\n",
      " [0.09545652 0.10638112 0.09342949 0.0994178  0.09095348 0.10847835\n",
      "  0.0934839  0.10919935 0.10425238 0.09894761]\n",
      " [0.09566064 0.1061196  0.09375346 0.0999237  0.09063712 0.10820829\n",
      "  0.09325563 0.10960978 0.10420615 0.09862564]\n",
      " [0.09566839 0.10626463 0.09351853 0.09969256 0.09069437 0.10820345\n",
      "  0.09344154 0.10940851 0.10394823 0.09915978]\n",
      " [0.09535221 0.10627275 0.09376893 0.09961016 0.09088352 0.10819843\n",
      "  0.09362027 0.10947123 0.10416897 0.09865354]\n",
      " [0.09558932 0.10627771 0.09366072 0.09980984 0.09046237 0.10851397\n",
      "  0.09327427 0.10930638 0.10425996 0.09884546]\n",
      " [0.09543407 0.10592751 0.09384887 0.09980418 0.09079711 0.10840115\n",
      "  0.09348634 0.10931614 0.10425167 0.09873296]\n",
      " [0.09581119 0.10605737 0.09334278 0.09972789 0.09090381 0.10835434\n",
      "  0.09326721 0.10950463 0.10417236 0.09885843]\n",
      " [0.09541844 0.1062889  0.09377378 0.09949083 0.09082979 0.10835912\n",
      "  0.09332233 0.10920379 0.10429452 0.0990185 ]\n",
      " [0.0954048  0.1064432  0.09387226 0.09949141 0.09082074 0.10867415\n",
      "  0.09312275 0.10945682 0.10430271 0.09841116]\n",
      " [0.09523279 0.10625551 0.09357709 0.09953996 0.09059246 0.10841053\n",
      "  0.09332437 0.10981466 0.10424206 0.09901056]\n",
      " [0.09564395 0.10632458 0.09391618 0.09946829 0.0911317  0.10816313\n",
      "  0.093232   0.10949045 0.10391122 0.0987185 ]\n",
      " [0.09535088 0.10646233 0.09389149 0.09983574 0.09093225 0.10809943\n",
      "  0.09324072 0.10937855 0.10425203 0.09855657]\n",
      " [0.09529169 0.10650031 0.09360916 0.09973552 0.09056604 0.10849623\n",
      "  0.09320222 0.10955347 0.10421614 0.09882921]\n",
      " [0.09546542 0.1062263  0.09352039 0.0994921  0.09075747 0.10855946\n",
      "  0.09323207 0.10950148 0.10440143 0.09884387]\n",
      " [0.0954118  0.1060949  0.09391295 0.09935037 0.09082711 0.10864302\n",
      "  0.09329964 0.10959248 0.10417588 0.09869186]\n",
      " [0.0953515  0.10650453 0.09337122 0.09982526 0.09076496 0.108319\n",
      "  0.09316052 0.10935254 0.10444656 0.09890392]\n",
      " [0.09590772 0.10644898 0.0935583  0.09969008 0.0906691  0.10818753\n",
      "  0.09337365 0.10901874 0.10435038 0.09879552]\n",
      " [0.0952124  0.10655665 0.09405407 0.09944479 0.09056922 0.10857671\n",
      "  0.09348035 0.10927748 0.10391277 0.09891556]\n",
      " [0.09542904 0.10683341 0.09367884 0.0997605  0.09059488 0.10811201\n",
      "  0.09313009 0.10937422 0.10395737 0.09912964]\n",
      " [0.09497166 0.1060952  0.09365741 0.09966509 0.09094659 0.10866033\n",
      "  0.09333384 0.10937456 0.10450495 0.09879037]\n",
      " [0.09579048 0.10633025 0.09349511 0.09952232 0.09074742 0.10822493\n",
      "  0.09328149 0.1095122  0.1041803  0.09891549]\n",
      " [0.09547166 0.10589494 0.09383092 0.09941583 0.0907216  0.10841768\n",
      "  0.09325769 0.10968516 0.10435922 0.0989453 ]\n",
      " [0.09529944 0.10661763 0.09332901 0.09969617 0.09077777 0.10842858\n",
      "  0.09351661 0.10948182 0.10399489 0.09885806]\n",
      " [0.09525254 0.10648686 0.09346012 0.09952261 0.09045981 0.10834047\n",
      "  0.09364211 0.10998222 0.10420919 0.09864406]\n",
      " [0.09576314 0.10643256 0.09358591 0.09937354 0.09042331 0.10837883\n",
      "  0.09313848 0.10967688 0.10419043 0.09903691]\n",
      " [0.09555993 0.1063493  0.09350556 0.09904968 0.09105707 0.10810122\n",
      "  0.09319823 0.1096591  0.10473848 0.09878143]\n",
      " [0.0952649  0.10622657 0.0937252  0.09994948 0.09073009 0.10840408\n",
      "  0.09318306 0.10947058 0.10416687 0.09887918]\n",
      " [0.09518374 0.10612413 0.09394419 0.09967531 0.09101469 0.1083975\n",
      "  0.0933483  0.10933855 0.10426146 0.09871212]\n",
      " [0.09559941 0.10644152 0.09356932 0.09971181 0.09033679 0.10852505\n",
      "  0.09336759 0.10944014 0.10397621 0.09903215]\n",
      " [0.09527367 0.10653345 0.093571   0.09963397 0.09085054 0.10848707\n",
      "  0.09324736 0.10940785 0.10431602 0.09867906]\n",
      " [0.09546631 0.10633416 0.09358627 0.09962683 0.09061291 0.10847693\n",
      "  0.09321197 0.10981087 0.10424128 0.09863246]\n",
      " [0.0956301  0.10621231 0.09361894 0.09947224 0.09095023 0.1083771\n",
      "  0.09335216 0.109549   0.10410068 0.09873725]\n",
      " [0.09566998 0.10623219 0.09367669 0.09962162 0.09063441 0.10841314\n",
      "  0.09312355 0.10960849 0.10412044 0.09889948]\n",
      " [0.0955693  0.10619908 0.09361933 0.09943878 0.09087458 0.10834612\n",
      "  0.09326927 0.10960582 0.10434353 0.09873418]\n",
      " [0.09572359 0.10616223 0.09360116 0.09973091 0.09043612 0.10835767\n",
      "  0.09327821 0.10976324 0.10401105 0.09893582]\n",
      " [0.09546676 0.10601177 0.09359208 0.0992169  0.09090261 0.10880174\n",
      "  0.09317948 0.10973319 0.1042874  0.09880808]\n",
      " [0.09522663 0.10613913 0.09363107 0.09961851 0.09107804 0.10814136\n",
      "  0.09358729 0.10968149 0.10411447 0.09878202]\n",
      " [0.09540689 0.10637681 0.0935029  0.09972508 0.09056024 0.10856045\n",
      "  0.09321054 0.10991473 0.10398538 0.09875697]\n",
      " [0.09562995 0.10651266 0.0933025  0.09933206 0.09073857 0.10845954\n",
      "  0.09322886 0.10953793 0.10443228 0.09882565]\n",
      " [0.09553574 0.10636026 0.09355377 0.099563   0.09062236 0.10818612\n",
      "  0.0933005  0.10963684 0.10442745 0.09881396]\n",
      " [0.09540885 0.10654146 0.09340506 0.09963267 0.09086551 0.10834308\n",
      "  0.09308172 0.1093981  0.10432939 0.09899415]\n",
      " [0.09543209 0.10645117 0.09359913 0.09938071 0.09063028 0.10841913\n",
      "  0.09336183 0.10972267 0.10427089 0.09873209]\n",
      " [0.09536999 0.10619976 0.09346875 0.09955754 0.09094971 0.10807539\n",
      "  0.09319651 0.10939231 0.1045137  0.09927635]\n",
      " [0.09539031 0.1062976  0.09362468 0.09952828 0.0905782  0.1084605\n",
      "  0.09301196 0.10974246 0.10450191 0.0988641 ]\n",
      " [0.09533287 0.10646064 0.0937842  0.09960267 0.09084375 0.1084283\n",
      "  0.09335316 0.10945047 0.10396788 0.09877605]\n",
      " [0.09567785 0.10649589 0.09407719 0.09936678 0.09059565 0.10833256\n",
      "  0.09311464 0.10938188 0.10439595 0.09856163]\n",
      " [0.09550825 0.10635742 0.09354574 0.09972342 0.09060916 0.10871446\n",
      "  0.093244   0.10942546 0.10424417 0.09862792]\n",
      " [0.09548372 0.1062437  0.09363301 0.09978698 0.09082772 0.10831656\n",
      "  0.09328262 0.10957639 0.10404867 0.09880062]\n",
      " [0.09560806 0.10619709 0.09385704 0.09961322 0.09063991 0.1085906\n",
      "  0.09317307 0.10938022 0.1041419  0.09879889]\n",
      " [0.09533606 0.10655206 0.093658   0.09968011 0.09054568 0.10839668\n",
      "  0.09339003 0.10926858 0.10422857 0.09894421]\n",
      " [0.09519492 0.10628079 0.09368569 0.09951151 0.0907881  0.10852262\n",
      "  0.09340569 0.10957663 0.10431816 0.09871589]\n",
      " [0.09549385 0.10640008 0.09357088 0.09946818 0.09054342 0.10853363\n",
      "  0.09315026 0.10991387 0.10393685 0.09898897]\n",
      " [0.09527609 0.10629989 0.09363752 0.09963215 0.0907178  0.10838561\n",
      "  0.09326824 0.10956209 0.104383   0.0988376 ]\n",
      " [0.09564081 0.10643318 0.09374845 0.09933119 0.09090968 0.10840318\n",
      "  0.09350794 0.10950995 0.10396191 0.0985537 ]\n",
      " [0.09554084 0.10624302 0.09370744 0.09964562 0.09074578 0.10812017\n",
      "  0.09342749 0.1096241  0.10422641 0.09871913]\n",
      " [0.09548241 0.10630066 0.09357475 0.09943892 0.09088291 0.10831853\n",
      "  0.09353087 0.10954063 0.10393457 0.09899576]\n",
      " [0.09562376 0.10649325 0.09363073 0.09971726 0.09073651 0.10814699\n",
      "  0.09326387 0.10981636 0.10380033 0.09877096]\n",
      " [0.09547003 0.10621887 0.09394724 0.09965912 0.09060278 0.108565\n",
      "  0.09344683 0.10951634 0.10407947 0.09849432]\n",
      " [0.0954779  0.10630281 0.09366975 0.09989065 0.09045089 0.10839082\n",
      "  0.09313241 0.10950239 0.10445995 0.09872242]\n",
      " [0.09567622 0.10626256 0.09390242 0.09943449 0.09078892 0.10834056\n",
      "  0.09332941 0.10965356 0.10410729 0.09850456]\n",
      " [0.09543193 0.10595925 0.09328562 0.09980311 0.09076241 0.10873461\n",
      "  0.09358788 0.10935953 0.10445364 0.09862202]\n",
      " [0.09560911 0.10637185 0.09400613 0.0991496  0.09087529 0.10822578\n",
      "  0.09328546 0.10971671 0.10390721 0.09885287]\n",
      " [0.09536137 0.10622669 0.09363789 0.09912346 0.09092767 0.10894958\n",
      "  0.09339267 0.1092624  0.10429304 0.09882523]\n",
      " [0.09525554 0.10638166 0.09389937 0.09975533 0.09047899 0.10875635\n",
      "  0.09326775 0.10961357 0.10415421 0.09843723]\n",
      " [0.09527987 0.10614745 0.09424108 0.09982274 0.09078106 0.10844826\n",
      "  0.09349167 0.10920619 0.10382504 0.09875663]\n",
      " [0.09543109 0.10634623 0.09373819 0.09997403 0.09072566 0.1082161\n",
      "  0.0934351  0.10944937 0.1042382  0.09844602]\n",
      " [0.0953735  0.10634272 0.09373713 0.09955231 0.09081937 0.10844818\n",
      "  0.09308536 0.10978017 0.10423652 0.09862473]\n",
      " [0.09528805 0.1061198  0.09353863 0.09967114 0.09056539 0.10860496\n",
      "  0.09330452 0.10996451 0.1041067  0.0988363 ]\n",
      " [0.09550219 0.10627242 0.09345947 0.09943779 0.09074812 0.1086632\n",
      "  0.09303009 0.10953844 0.10442996 0.09891833]\n",
      " [0.09498984 0.10647327 0.09382777 0.09945247 0.09088428 0.10864714\n",
      "  0.09321865 0.10985376 0.10425461 0.09839822]\n",
      " [0.09543103 0.10603063 0.09348181 0.09953155 0.09080063 0.10858077\n",
      "  0.09372864 0.10952439 0.10405881 0.09883175]\n",
      " [0.09570446 0.10629067 0.09362274 0.09942892 0.09049368 0.1081158\n",
      "  0.09350399 0.10960719 0.10439671 0.09883583]\n",
      " [0.09568612 0.10594507 0.09373508 0.09954655 0.09073958 0.10820332\n",
      "  0.09348035 0.10976549 0.10418982 0.09870863]\n",
      " [0.09567408 0.10611237 0.09360317 0.09970395 0.09065025 0.10810519\n",
      "  0.09314401 0.10980152 0.1043927  0.09881276]]\n"
     ]
    }
   ],
   "source": [
    "x = np.random.rand(100, 784) # 더미 입력 데이터 (100장 분량)\n",
    "y = net.predict(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "12a4bade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 100)\n",
      "(100,)\n",
      "(100, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "x = np.random.rand(100,784) # 더미 입력 데이터 100장 분량\n",
    "t = np.random.rand(100,10) # 더미 정답 데이터 100장 분량\n",
    "\n",
    "grads = net.numerical_gradient(x,t)\n",
    "\n",
    "print(grads['W1'].shape)\n",
    "print(grads['b1'].shape)\n",
    "print(grads['W2'].shape)\n",
    "print(grads['b2'].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd5827e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4719cbfc",
   "metadata": {},
   "source": [
    "`미니배치 학습 구현`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "da72c5a9",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-106-93bd71ebb7a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# 기울기 계산\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumerical_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0;31m# gard = network.gradient(x_batch, t_batch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-104-721de38bc0d2>\u001b[0m in \u001b[0;36mnumerical_gradient\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumerical_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumerical_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W2'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumerical_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-103-428498e12851>\u001b[0m in \u001b[0;36mnumerical_gradient\u001b[0;34m(f, x)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_val\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mfxh2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# f(x-h)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mgrad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfxh1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mfxh2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-104-721de38bc0d2>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(W)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnumerical_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mloss_W\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mW\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-104-721de38bc0d2>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcross_entropy_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-104-721de38bc0d2>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mz1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0ma2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-102-de1129a4f4c6>\u001b[0m in \u001b[0;36msoftmax\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "(x_train, t_train), (y_train, t_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "\n",
    "train_loss_list = []\n",
    "\n",
    "#하이퍼파라미터\n",
    "\n",
    "iters_num = 10000 #반복 횟수\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100 # 미니배치 크기\n",
    "learning_rate = 0.1\n",
    "network = TwoLayerNet(input_size= 784, hidden_size=100, output_size=10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    #미니배치 획득\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 기울기 계산\n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    # gard = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    # 매개변수 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "        \n",
    "    #학습 경과 기록\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259c62fa",
   "metadata": {},
   "source": [
    "    - 미니배치 크기를 100으로 하여, 매번 60,000개의 훈련 데이터에서 임의로 100개의 데이터(이미지 데이터와 정답 레이블 데이터)를 추려냄\n",
    "    - 그 100개의 미니배치를 대상으로 확률적 경사 하강법을 수행해 매개변수를 갱신\n",
    "    - 경사법에 의한 갱신 횟수(반복 횟수)를 10,000번으로 설정하고, 갱신할 때마다 훈련 데이터에 대한 손실 함수를 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6f6e8e",
   "metadata": {},
   "source": [
    "`시험 데이터로 평가`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "be61cd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "    \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "        \n",
    "    def gradient(self, x, t):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        grads = {}\n",
    "        \n",
    "        batch_num = x.shape[0]\n",
    "        \n",
    "        # forward\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        # backward\n",
    "        dy = (y - t) / batch_num\n",
    "        grads['W2'] = np.dot(z1.T, dy)\n",
    "        grads['b2'] = np.sum(dy, axis=0)\n",
    "        \n",
    "        da1 = np.dot(dy, W2.T)\n",
    "        dz1 = sigmoid_grad(a1) * da1\n",
    "        grads['W1'] = np.dot(x.T, dz1)\n",
    "        grads['b1'] = np.sum(dz1, axis=0)\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "36cd87fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "AxisError",
     "evalue": "axis 1 is out of bounds for array of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-117-83b55df2934a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;31m# 1에폭당 정확도 계산\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0miter_per_epoch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mtrain_acc_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-107-50ed366b366a>\u001b[0m in \u001b[0;36maccuracy\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36margmax\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36margmax\u001b[0;34m(a, axis, out)\u001b[0m\n\u001b[1;32m   1191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m     \"\"\"\n\u001b[0;32m-> 1193\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'argmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 1"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "(x_train, t_train), (y_train, t_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "y_train = y_train.reshape(10000, 784)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "\n",
    "#하이퍼 파라미터\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size =100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "# 1에폭당 반복 수\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # 미니 배치 획득\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 기울기 계산\n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    # grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    # 매개변수 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "        \n",
    "    # 학습 경과 기록\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    # 1에폭당 정확도 계산\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print('train acc, test acc |' + str(train_acc) + ', ' + str(test_acc)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0f7dbc",
   "metadata": {},
   "source": [
    "     - 1 에폭마다 모든 훈련 데이터와 시험 데이터에 대한 정확도 계산 \n",
    "     - 정확도를 1에폭마다 계산하는 이유는 for 문안에서 매번 게산하기에는 시간이 오래걸리고, 자주 기록할 필요가 없기 때문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2053da9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.32915135, -0.23250927,  0.69811818],\n",
       "       [ 1.04279034, -1.17171146, -1.15089516]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d5f00c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
